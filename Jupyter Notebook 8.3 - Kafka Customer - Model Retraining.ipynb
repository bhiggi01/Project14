{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2556c415",
   "metadata": {},
   "source": [
    "# Kafka Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67754416",
   "metadata": {},
   "source": [
    "A notebook that is used on the local machine to connect to the Kafka server and act as the consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae7116",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f8689",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f94328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T12:24:27.258050Z",
     "start_time": "2023-11-22T12:24:25.118425Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries in the Consumer Notebook\n",
    "\n",
    "# General\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SQL\n",
    "import pymysql\n",
    "\n",
    "# Data types\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import avro\n",
    "\n",
    "# Avro objects\n",
    "import io\n",
    "import fastavro\n",
    "\n",
    "# Kafka\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "\n",
    "# Machine Learning Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5207f52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c494ff",
   "metadata": {},
   "source": [
    "# Fix Issue with Consumer not serailising issue correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841bcd5",
   "metadata": {},
   "source": [
    "There was an issue where the ....."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c4c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T11:18:04.278460Z",
     "start_time": "2023-11-21T11:18:04.278460Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9782122f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:27:52.048864Z",
     "start_time": "2023-11-01T18:27:48.259993Z"
    }
   },
   "source": [
    "# Basic Consumer test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba2aeb",
   "metadata": {},
   "source": [
    "Start the consumer first to be listening for a producer message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1613b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.255665Z",
     "start_time": "2023-11-21T22:28:30.240577Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Kafka configuration\n",
    "# conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "#        'group.id':'jupyter-consumer-group',\n",
    "#        'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# # Create a consumer instance\n",
    "# consumer = Consumer(conf)\n",
    "\n",
    "# # Subscribe to the topic\n",
    "# consumer.subscribe(['network-data-events'])\n",
    "\n",
    "# # Use a try, except, finally to allow kafka consumer to shut down gracefully.\n",
    "# # Try, continuously polls for new messages. Start the porducer to get a messages sent to the consumer\n",
    "# # except, lets you handle errors, \n",
    "# # Finally, lets you execte code. Lets the cosumer code be closed.\n",
    "\n",
    "# try:\n",
    "#     # Code block to poll messages from the producer\n",
    "#     while True:\n",
    "#         msg = consumer.poll(timeout=1.0)\n",
    "#         if msg is None: continue\n",
    "#         if msg.error():\n",
    "#             if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "#                 # End of partition event\n",
    "#                 print(f'{msg.topic()}[{msg.partition()}] reached end at offset {msg.offset()}')\n",
    "#             elif msg.error():\n",
    "#                 raise KafkaException(msg.error())\n",
    "#         else:\n",
    "#             print(f'Recieved message: {msg.value().decode(\"utf-8\")}')\n",
    "# except KeybaordInterrupt:\n",
    "#     pass\n",
    "# finally:\n",
    "#     # Closes the connection\n",
    "#     consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74575e19",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a2083",
   "metadata": {},
   "source": [
    "# Real Time Anomaly Detection Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1ba811",
   "metadata": {},
   "source": [
    "This consumer will take in the two topics from the Producer that have the features and the labels that have been separated out into two topics. Every 5th batch the second topic will be used to test the results to get an idea how the model is performing.\n",
    "\n",
    "This is done as a first test and to reduce computer power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef748c0",
   "metadata": {},
   "source": [
    "# Run before running Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74199ae",
   "metadata": {},
   "source": [
    "### SQL connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812b293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.270981Z",
     "start_time": "2023-11-21T22:28:30.257660Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Function to connect to MYSQL connection\n",
    "# def mysql_connection():\n",
    "#     return pymysql.connect(host='localhost',\n",
    "#                             user='root',\n",
    "#                             password='root',\n",
    "#                             db='mtu_capstone_db')            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244098a",
   "metadata": {},
   "source": [
    "### Load schema for features and labels. Created in the Producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8985f17",
   "metadata": {},
   "source": [
    "##### Attempts to load schema from file. Can cause issues if not loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49291472",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T12:24:31.614134Z",
     "start_time": "2023-11-22T12:24:31.602620Z"
    }
   },
   "outputs": [],
   "source": [
    "# ###############################\n",
    "# # Attempt 4: Use Fastavro to read schema.\n",
    "# ###############################\n",
    "def load_avro_schema_with_fastavro(schema_file_path):\n",
    "    schema = fastavro.schema.load_schema(schema_file_path)\n",
    "    return schema\n",
    "\n",
    "# folder location and file names for the schema files\n",
    "folder_path = r'C:\\Users\\Kolobane\\OneDrive\\CIT MSc Data Science Modules\\_Semester Three - Final Project\\Project Two - Network Project\\Data\\Avro Schema'\n",
    "features_avro_schema_file = \"features_avro_schema.avsc\" \n",
    "label_avro_schema_file = \"label_avro_schema.avsc\"\n",
    "\n",
    "# Call each scehma\n",
    "features_avro_schema = load_avro_schema_with_fastavro(os.path.join(folder_path ,features_avro_schema_file)) \n",
    "label_avro_schema = load_avro_schema_with_fastavro(os.path.join(folder_path, label_avro_schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e49807d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.302374Z",
     "start_time": "2023-11-21T22:28:30.287699Z"
    }
   },
   "outputs": [],
   "source": [
    "# # features_avro_schema\n",
    "# label_avro_schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a45fea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.317906Z",
     "start_time": "2023-11-21T22:28:30.303434Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# features_avro_schema "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1c4a4",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98644e",
   "metadata": {},
   "source": [
    "Models are too large to save, they will be 600mb plus times 50. I don't have the resources for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1890bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.333842Z",
     "start_time": "2023-11-21T22:28:30.318772Z"
    }
   },
   "outputs": [],
   "source": [
    "# def save_model(model, filename):\n",
    "#     model_path = os.path.join('C:\\\\Users\\\\Kolobane\\\\OneDrive\\\\CIT MSc Data Science Modules\\\\_Semester Three - Final Project\\\\Project Two - Network Project\\\\ML Models\\\\Random Forest Real Time Models', filename)\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52eee6e",
   "metadata": {},
   "source": [
    "### Function to deserialise avro record sent from the Producer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f550ac3",
   "metadata": {},
   "source": [
    "##### Attempt 1. Didnt work. Match error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfc710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.349634Z",
     "start_time": "2023-11-21T22:28:30.334893Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Take the avro objects from the producer and with the schema, remove the data for use.\n",
    "# def deserialise_avro_record(avro_bytes, avro_schema):\n",
    "#     # Create a datum reader using the schema avro\n",
    "#     reader = DatumReader(avro_schema)\n",
    "    \n",
    "#     # Create a a Binary Decoder with avro bytes\n",
    "#     decoder = BinaryDecoder(io.BytesIO(avro_bytes))\n",
    "    \n",
    "#     return reader.read(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf7e78",
   "metadata": {},
   "source": [
    "##### Attempt 2: Create two functions, by listing out the featues in the DatumReader did not work. Deleted code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96035188",
   "metadata": {},
   "source": [
    "###### Attempt 3: Getting a \"Received message from topic: batch-network-data, key: b'0'. Error: unhashable type: 'RecordSchema'\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1784252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.365497Z",
     "start_time": "2023-11-21T22:28:30.350640Z"
    }
   },
   "outputs": [],
   "source": [
    "# def deserialise_features_avro_record(avro_bytes, features_avro_schema):\n",
    "#     # Create a by bytesIO steam from the avro bytes\n",
    "#     bytes_reader = io.BytesIO(avro_bytes)\n",
    "    \n",
    "#     # deserailise the data using the schema\n",
    "#     features_data = schemaless_reader(bytes_reader, features_avro_schema)\n",
    "    \n",
    "#     return features_data\n",
    "\n",
    "# def deserialise_label_avro_record(avro_bytes, label_avro_schema):\n",
    "#     # Create a by bytesIO steam from the avro bytes\n",
    "#     bytes_reader = io.BytesIO(avro_bytes)\n",
    "    \n",
    "#     # deserailise the data using the schema\n",
    "#     label_data = schemaless_reader(bytes_reader, label_avro_schema)\n",
    "    \n",
    "#     return label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e77c9",
   "metadata": {},
   "source": [
    "##### Attempt 4: Add in debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc81656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T12:24:34.614949Z",
     "start_time": "2023-11-22T12:24:34.602430Z"
    }
   },
   "outputs": [],
   "source": [
    "def deserialise_features_avro_record(avro_bytes, schema):\n",
    "    bytes_reader = io.BytesIO(avro_bytes)\n",
    "    deserialised_data = []\n",
    "    for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "        deserialised_data.append(record)\n",
    "    return deserialised_data\n",
    "\n",
    "def deserialise_label_avro_record(avro_bytes, schema):\n",
    "    bytes_reader = io.BytesIO(avro_bytes)\n",
    "    deserialised_data = []\n",
    "    for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "        deserialised_data.append(record)\n",
    "    return deserialised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aac335",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cac3894f",
   "metadata": {},
   "source": [
    "## Run Consumer 1: Random Forest Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e170086",
   "metadata": {},
   "source": [
    "Set up consumer to listen for the batches sent from the producer. Loads in baseline model, tests the batches, outputs results to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe15a65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.397249Z",
     "start_time": "2023-11-21T22:28:30.382717Z"
    }
   },
   "outputs": [],
   "source": [
    "# ############################################################\n",
    "# # Set up the Kafka configuration\n",
    "# ############################################################\n",
    "# # Kafka configuration\n",
    "# conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "#        'group.id':'jupyter-consumer-group',\n",
    "#        'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# # Create Kafka Consumer instance\n",
    "# consumer = Consumer(conf)\n",
    "\n",
    "# # Subscribe to the topics\n",
    "# consumer.subscribe([\"batch-network-data\", \"test-batch-labels\"]) # consumer needs to listen to both topics.\n",
    "\n",
    "# ############################################################\n",
    "# # Load the baseline model: Random Forest\n",
    "# ############################################################\n",
    "# # # Model Location\n",
    "# # model_path = r\"C:\\Users\\Kolobane\\OneDrive\\CIT MSc Data Science Modules\\_Semester Three - Final Project\\Project Two - Network Project\\ML Models\\rf_model_baseline_basic.joblib\"\n",
    "\n",
    "# # # Load the previous pre-trained baseline model.\n",
    "# # model = joblib.load(model_path)\n",
    "\n",
    "# ############################################################\n",
    "# # Random Forest model parameters\n",
    "# ############################################################\n",
    "# # # For each model I will use the same parameters as I used in the previous models. For RF basic model, thats n_estimators=100\n",
    "# # new_model_hyperparameters = {\n",
    "# #     'n_estimators': 100\n",
    "# # }\n",
    "\n",
    "# ############################################################\n",
    "# # Initialize dictionaries for storing batch data\n",
    "# ############################################################\n",
    "# batch_features = {}\n",
    "# batch_labels = {}\n",
    "\n",
    "# ############################################################\n",
    "# # Start Consumer polling\n",
    "# ############################################################\n",
    "# try: \n",
    "#     while True:\n",
    "#         msg = consumer.poll(1.0) # poll the first topic\n",
    "        \n",
    "#         if msg is None:\n",
    "#             continue\n",
    "#         if msg.error():\n",
    "#             print(f\"Consumer error: {msg.error()}\")\n",
    "#             continue   \n",
    "    \n",
    "#         ############################################################\n",
    "#         # Check Topics - this is kept seperate for the next models.\n",
    "#         ############################################################\n",
    "    \n",
    "#         #######################\n",
    "#         # Check if topics are from \"batch-network-data\"\n",
    "#         if msg.topic() == \"batch-network-data\":\n",
    "#             batch_number = int(msg.key()) # get the batch number\n",
    "#             avro_bytes = msg.value()\n",
    "            \n",
    "#             # Deserilise the avro data with the label_avro_schema loaded above\n",
    "#             avro_data = deserialize_avro_record(avro_bytes, features_avro_schema )\n",
    "#             batch_features[batch_number] = avro_data\n",
    "            \n",
    "    \n",
    "#         #######################\n",
    "#         # Check if topics are from \"test-batch-labels\"\n",
    "#         if msg.topic() == \"test-batch-labels\":\n",
    "#             batch_number = int(msg.key())\n",
    "#             avro_bytes = msg.value()\n",
    "            \n",
    "#             # Deserilise the avro data with the features_avro_schema loaded above\n",
    "#             avro_data = deserialize_avro_record(avro_bytes,label_avro_schema)\n",
    "#             batch_features[batch_number] = avro_data\n",
    "            \n",
    "\n",
    "#         ############################################################\n",
    "#         # Predict Using Baseline Models\n",
    "#         ############################################################\n",
    "    \n",
    "# #         #######################\n",
    "# #         # Use baseline model to make predictions with current batch\n",
    "# #         if batch_number in batch_features and batch_number in batch_labels:\n",
    "# #             batch_features_data = batch_features[batch_number]\n",
    "# #             batch_labels_data = batch_labels[batch_number]\n",
    "            \n",
    "# #             # Make predictions using the baseline model\n",
    "# #             baseline_predictions = model.predict(preprocessed_data)\n",
    "        \n",
    "# #         else:\n",
    "# #             print(\"Batch features or labels are missing for batch number:\", batch_number)\n",
    "        \n",
    "#         ############################################################\n",
    "#         # Combine Features and Labels\n",
    "#         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         # Combine Features and the labels.\n",
    "# #         if batch_features_data and batch_labels_data:\n",
    "# #             combined_data = {\n",
    "# #                 \"features\": batch_features_data,\n",
    "# #                 \"labels\": batch_labels_data\n",
    "# #             }\n",
    "# #         else:\n",
    "# #             print(\"Batch features or labels are missing for batch number:\", batch_number)\n",
    "        \n",
    "#         ############################################################\n",
    "#         # Evaluate Predictions\n",
    "#         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## Compare the baseline model with the actual labels.\n",
    "# #         if batch_number in baseline_predictios and batch_number in batch_labels:\n",
    "# #             baseline_predictions = baseline_predictions_dict[batch_number]\n",
    "# #             actual_labels = batch_labels[batch_number]\n",
    "            \n",
    "# #             if baseline_predictions and actual_labels:\n",
    "                \n",
    "# #                 # Metrics\n",
    "# #                 accuracy_value = accuracy_score(actual_labels, baseline_predictions)\n",
    "# #                 precision_value = precision_score(actual_labels, baseline_predictions)\n",
    "# #                 recall_value = recall_score(actual_labels, baseline_predictions)\n",
    "# #                 f1_score_value = f1_score(actual_labels, baseline_predictions)\n",
    "# #                 auc_score = roc_auc_score(actual_labels, baseline_predictions)\n",
    "                \n",
    "# #                 # Confusion matrix\n",
    "# #                 confusion_matrix_result = confusion_matrix(actual_labels, baseline_predictions)\n",
    "# #                 confusion_matrix = json.dumps(confusion_matrix, matrix_result.tolist())\n",
    "                \n",
    "# #                 # feature Importance\n",
    "# #                 feature_importance_results = model.feature_importances_\n",
    "# #                 feature_importance = json.dumps(feature_importance_results)           \n",
    "                \n",
    "# #             else:\n",
    "# #                 print(\"Batch data is empty for batch_number:\", batch_number)\n",
    "        \n",
    "# #         else:\n",
    "# #             print(\"Batch predictions or labels missing in batch_number:\", batch_number)\n",
    "        \n",
    "        \n",
    "# #         ############################################################\n",
    "# #         # Retrain the baseline model to get the next baseline model.\n",
    "# #         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## retain model to get new baseline model\n",
    "# #         retrain_start_time = time.time()\n",
    "        \n",
    "# #         # Create a new baseline model with the same hyperparameters\n",
    "# #         new_baseline_model = RandomForestClassifier(**new_model_hyperparameters)\n",
    "        \n",
    "# #         # Retrain the baseline model wiht the batch data\n",
    "# #         new_baseline_model.fit(batch_features_data, batch_labels_data)\n",
    "        \n",
    "# #         # Stop the timer\n",
    "# #         retrain_end_time =  time.time()\n",
    "        \n",
    "# #         #######################\n",
    "# #         # save date\n",
    "# #         model_training_time_seconds = retrain_end_time -retrain_start_time\n",
    "        \n",
    "# #         # save the new model parameters # although I have decided to use the same.\n",
    "# #         new_model_parameters_results = new_baseline_model.get_params()\n",
    "# #         new_model_parameters_results_tolist = new_model_parameters_results.tolist()\n",
    "# #         new_model_parameters = json.dumps(new_model_parameters_results_tolist)\n",
    "        \n",
    "# #         ############################################################\n",
    "# #         # Send values to database\n",
    "# #         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## connect to the database\n",
    "# #         conn = mysql_connection()        \n",
    "# #         cursor = conn.cursor()\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## Create SQL Query\n",
    "# #         insert_query = \"\"\"\n",
    "# #             INSERT INTO rf_basic_rt_model_results(\n",
    "# #                 batch_number, model_name, timestamp, accuracy_value,\n",
    "# #                 precision_value, recall_value, f1_value, auc_score,\n",
    "# #                 confusion_matrix, feature_importance, testing_time_seconds,\n",
    "# #                 model_training_time_seconds, model_parameters\n",
    "# #             )\n",
    "# #             VALUES (%s, %s, NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s ) \"\"\"\n",
    "\n",
    "# #         # Values to be insert#\n",
    "# #         values =  (\n",
    "# #             batch_number, model_name, accuracy_value,\n",
    "# #             precision_value, recall_value, f1_value, auc_score,\n",
    "# #             confusion_matrix_json, feature_importance_json, testing_time_seconds,\n",
    "# #             retraining_time_seconds, new_model_parameters_json\n",
    "# #         )\n",
    "\n",
    "# #         # Execute the insert query that is defind above\n",
    "# #         cursor.execute(insert_query, values)\n",
    "        \n",
    "# #         # Commit the changes to the database\n",
    "# #         conn.commit()\n",
    "        \n",
    "# #         print(f\"Batch {batch_number} values inserted successfully into the database.\")\n",
    "       \n",
    "              \n",
    "# ############################################################\n",
    "# # Error handling and logging\n",
    "# ############################################################             \n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "    \n",
    "# ############################################################\n",
    "# # Clean up and close\n",
    "# ############################################################        \n",
    "# finally:\n",
    "#     # Good practices to reduce issues by cleaning up and closing connections\n",
    "              \n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "#         conn.close()\n",
    "        \n",
    "#     if \"cursor\" in locals() or \"conn\" in globals():   \n",
    "#         cursor.close()\n",
    "    \n",
    "#     consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a79e1d",
   "metadata": {},
   "source": [
    "### Shorter version of consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e10558",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-22T12:24:39.160Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received message from topic: batch-network-data, key: b'0'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################\n",
    "# Set up the Kafka configuration\n",
    "############################################################\n",
    "# Kafka configuration\n",
    "conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "       'group.id':'jupyter-consumer-group',\n",
    "       'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# Create Kafka Consumer instance\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topics\n",
    "consumer.subscribe([\"batch-network-data\", \"test-batch-labels\"]) # consumer needs to listen to both topics.\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Initialize dictionaries for storing batch data\n",
    "############################################################\n",
    "batch_features = {}\n",
    "batch_label = {}\n",
    "\n",
    "############################################################\n",
    "# Start Consumer polling\n",
    "############################################################\n",
    "try: \n",
    "    while True:\n",
    "        msg = consumer.poll(1.0) # poll the first topic\n",
    "        \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue   \n",
    "    \n",
    "        # Debugging: Print the topic and keys\n",
    "        print(f\"Received message from topic: {msg.topic()}, key: {msg.key()}\")\n",
    "        \n",
    "        ############################################################\n",
    "        # Check Topics - this is kept seperate for the next models.\n",
    "        ############################################################\n",
    "    \n",
    "        #######################\n",
    "        # Check if topics are from \"batch-network-data\"\n",
    "        if msg.topic() == \"batch-network-data\":\n",
    "            batch_number = int(msg.key()) # get the batch number\n",
    "            avro_bytes = msg.value()\n",
    "            \n",
    "            # Deserilise the avro data with the label_avro_schema loaded above\n",
    "            avro_data = deserialise_features_avro_record(avro_bytes, features_avro_schema )\n",
    "            batch_features[batch_number] = avro_data\n",
    "            \n",
    "            # Debugging: print part of producer data\n",
    "            print(f\"Deserialized data for FEATURES: {avro_data}\")  \n",
    "            \n",
    "        #######################\n",
    "        # Check if topics are from \"test-batch-labels\"\n",
    "        if msg.topic() == \"test-batch-labels\":\n",
    "            batch_number = int(msg.key())\n",
    "            avro_bytes = msg.value()\n",
    "            \n",
    "            # Deserilise the avro data with the features_avro_schema loaded above\n",
    "            avro_data = deserialise_label_avro_record(avro_bytes, label_avro_schema)\n",
    "            batch_label[batch_number] = avro_data\n",
    "\n",
    "            # Debugging: print part of producer data\n",
    "            print(f\"Deserialized data for LABELS: {avro_data}\")\n",
    "        \n",
    "\n",
    "############################################################\n",
    "# Error handling and logging\n",
    "############################################################             \n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")   \n",
    "    \n",
    "############################################################\n",
    "# Clean up and close\n",
    "# ############################################################        \n",
    "# finally:\n",
    "#     # Good practices to reduce issues by cleaning up and closing connections\n",
    "              \n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "#         conn.close()\n",
    "        \n",
    "#     if \"cursor\" in locals() or \"conn\" in globals():   \n",
    "#         cursor.close()\n",
    "    \n",
    "#     consumer.close()\n",
    "\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ad765",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4a782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17057842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a915fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab65c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a39f6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e4bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb7814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8fd62c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8206b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "119f6cb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54209c8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f91a7",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2b65d",
   "metadata": {},
   "source": [
    "# Total Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee982f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.971756Z",
     "start_time": "2023-11-21T22:28:30.895425Z"
    }
   },
   "outputs": [],
   "source": [
    "# ###############################\n",
    "# Consumer code\n",
    "# ###############################\n",
    "\n",
    "############################################################\n",
    "def load_avro_schema_with_fastavro(schema_file_path):\n",
    "    schema = fastavro.schema.load_schema(schema_file_path)\n",
    "    return schema\n",
    "\n",
    "# folder location and file names for the schema files\n",
    "folder_path = r'C:\\Users\\Kolobane\\OneDrive\\CIT MSc Data Science Modules\\_Semester Three - Final Project\\Project Two - Network Project\\Data\\Avro Schema'\n",
    "features_avro_schema_file = \"features_avro_schema.avsc\" \n",
    "label_avro_schema_file = \"label_avro_schema.avsc\"\n",
    "\n",
    "# Call each scehma\n",
    "features_avro_schema = load_avro_schema_with_fastavro(os.path.join(folder_path ,features_avro_schema_file)) \n",
    "label_avro_schema = load_avro_schema_with_fastavro(os.path.join(folder_path, label_avro_schema_file))\n",
    "\n",
    "############################################################\n",
    "def deserialise_features_avro_record(avro_bytes, schema):\n",
    "    bytes_reader = io.BytesIO(avro_bytes)\n",
    "    deserialised_data = []\n",
    "    for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "        deserialised_data.append(record)\n",
    "    return deserialised_data\n",
    "\n",
    "def deserialise_label_avro_record(avro_bytes, schema):\n",
    "    bytes_reader = io.BytesIO(avro_bytes)\n",
    "    deserialised_data = []\n",
    "    for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "        deserialised_data.append(record)\n",
    "    return deserialised_data\n",
    "\n",
    "############################################################\n",
    "    \n",
    "############################################################\n",
    "# Set up the Kafka configuration\n",
    "############################################################\n",
    "# Kafka configuration\n",
    "conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "       'group.id':'jupyter-consumer-group',\n",
    "       'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# Create Kafka Consumer instance\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topics\n",
    "consumer.subscribe([\"batch-network-data\", \"test-batch-labels\"]) # consumer needs to listen to both topics.\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Initialize dictionaries for storing batch data\n",
    "############################################################\n",
    "batch_features = {}\n",
    "batch_label = {}\n",
    "\n",
    "############################################################\n",
    "# Start Consumer polling\n",
    "############################################################\n",
    "try: \n",
    "    while True:\n",
    "        msg = consumer.poll(1.0) # poll the first topic\n",
    "        \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue   \n",
    "    \n",
    "        # Debugging: Print the topic and keys\n",
    "        print(f\"Received message from topic: {msg.topic()}, key: {msg.key()}\")\n",
    "        \n",
    "        ############################################################\n",
    "        # Check Topics - this is kept seperate for the next models.\n",
    "        ############################################################\n",
    "    \n",
    "        #######################\n",
    "        # Check if topics are from \"batch-network-data\"\n",
    "        if msg.topic() == \"batch-network-data\":\n",
    "            batch_number = int(msg.key()) # get the batch number\n",
    "            avro_bytes = msg.value()\n",
    "            \n",
    "            # Deserilise the avro data with the label_avro_schema loaded above\n",
    "            avro_data = deserialise_features_avro_record(avro_bytes, features_avro_schema )\n",
    "            batch_features[batch_number] = avro_data\n",
    "            \n",
    "            # Debugging: print part of producer data\n",
    "            print(f\"Deserialized data for FEATURES: {avro_data}\")  \n",
    "            \n",
    "        #######################\n",
    "        # Check if topics are from \"test-batch-labels\"\n",
    "        if msg.topic() == \"test-batch-labels\":\n",
    "            batch_number = int(msg.key())\n",
    "            avro_bytes = msg.value()\n",
    "            \n",
    "            # Deserilise the avro data with the features_avro_schema loaded above\n",
    "            avro_data = deserialise_label_avro_record(avro_bytes, label_avro_schema)\n",
    "            batch_label[batch_number] = avro_data\n",
    "\n",
    "            # Debugging: print part of producer data\n",
    "            print(f\"Deserialized data for LABELS: {avro_data}\")\n",
    "        \n",
    "\n",
    "############################################################\n",
    "# Error handling and logging\n",
    "############################################################             \n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")   \n",
    "    \n",
    "############################################################\n",
    "# Clean up and close\n",
    "# ############################################################        \n",
    "# finally:\n",
    "#     # Good practices to reduce issues by cleaning up and closing connections\n",
    "              \n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "#         conn.close()\n",
    "        \n",
    "#     if \"cursor\" in locals() or \"conn\" in globals():   \n",
    "#         cursor.close()\n",
    "    \n",
    "#     consumer.close()\n",
    "\n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452b8a8",
   "metadata": {},
   "source": [
    "# Explore other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e98b259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e52f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bscapstone)",
   "language": "python",
   "name": "bscapstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "333.767px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
