{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2556c415",
   "metadata": {},
   "source": [
    "# Kafka Customer - Data Ingestion and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67754416",
   "metadata": {},
   "source": [
    "A notebook that is used on the local machine to connect to the Kafka server and act as the consumer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae7116",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9f8689",
   "metadata": {},
   "source": [
    "##### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f94328",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:58:29.429030Z",
     "start_time": "2023-11-23T16:58:27.375054Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries in the Consumer Notebook\n",
    "\n",
    "# General\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Data Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SQL\n",
    "import pymysql\n",
    "\n",
    "# Data types\n",
    "import json\n",
    "import pickle\n",
    "import joblib\n",
    "import avro\n",
    "\n",
    "# Avro objects\n",
    "import io\n",
    "import fastavro\n",
    "\n",
    "# Kafka\n",
    "from confluent_kafka import Consumer, KafkaException, KafkaError\n",
    "\n",
    "# Machine Learning Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a0504e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T13:26:42.117742Z",
     "start_time": "2023-11-24T13:26:42.110388Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5207f52",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a2083",
   "metadata": {},
   "source": [
    "# Data Ingestion and Storage Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bed175",
   "metadata": {},
   "source": [
    "## Load schema for features and labels. Created in the Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8020d3bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:58:31.000786Z",
     "start_time": "2023-11-23T16:58:30.990286Z"
    }
   },
   "outputs": [],
   "source": [
    "# ###############################\n",
    "def load_avro_schema_with_fastavro(schema_file_path):\n",
    "    schema = fastavro.schema.load_schema(schema_file_path)\n",
    "    return schema\n",
    "\n",
    "# folder location and file names for the schema files\n",
    "folder_path = r'C:\\Users\\Kolobane\\OneDrive\\CIT MSc Data Science Modules\\_Semester Three - Final Project\\Project Two - Network Project\\Data\\Avro Schema'\n",
    "features_avro_schema_file = \"features_avro_schema.avsc\" \n",
    "label_avro_schema_file = \"label_avro_schema.avsc\"\n",
    "\n",
    "# Call each scehma\n",
    "features_avro_schema = load_avro_schema_with_fastavro(os.path.join(folder_path ,features_avro_schema_file)) \n",
    "label_avro_schema = load_avro_schema_with_fastavro(os.path.join(folder_path, label_avro_schema_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3329f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # features_avro_schema\n",
    "# label_avro_schema "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45ee23",
   "metadata": {},
   "source": [
    "# Functions to deserialise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a62b996d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T16:58:33.779750Z",
     "start_time": "2023-11-23T16:58:33.758684Z"
    }
   },
   "outputs": [],
   "source": [
    "def deserialise_features_avro_record(avro_bytes, schema):\n",
    "    bytes_reader = io.BytesIO(avro_bytes)\n",
    "    deserialised_data = []\n",
    "    for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "        deserialised_data.append(record)\n",
    "    return deserialised_data\n",
    "\n",
    "def deserialise_label_avro_record(avro_bytes, schema):\n",
    "    bytes_reader = io.BytesIO(avro_bytes)\n",
    "    deserialised_data = []\n",
    "    for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "        deserialised_data.append(record)\n",
    "    return deserialised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641718ee",
   "metadata": {},
   "source": [
    "## Consumer to save message to file from the producer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6221da81",
   "metadata": {},
   "source": [
    "**NOTE:** - Update Location of saved files each time for step by step build of project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d4061fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-19T18:43:56.988971Z",
     "start_time": "2023-12-19T18:43:56.969416Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Set up the Kafka configuration\n",
    "############################################################\n",
    "# Kafka configuration\n",
    "conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "       'group.id':'jupyter-consumer-group',\n",
    "       'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# Create Kafka Consumer instance\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the topics\n",
    "consumer.subscribe([\"batch-network-data\", \"test-batch-labels\"]) # consumer needs to listen to both topics.\n",
    "\n",
    "############################################################\n",
    "# Save batches to folder\n",
    "############################################################\n",
    "# Location to store the producer messages.\n",
    "folder_path =  r\"C:\\Users\\Kolobane\\OneDrive\\CIT MSc Data Science Modules\\_Semester Three - Final Project\\Project Two - Network Project\\Data\\Consumer Data\\Avro Batches 2500 avro files\"\n",
    "\n",
    "############################################################\n",
    "# Start Consumer polling\n",
    "############################################################\n",
    "try: \n",
    "    while True:\n",
    "        msg = consumer.poll(1.0) # poll the first topic\n",
    "        \n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue   \n",
    "        \n",
    "        ############################################################\n",
    "        # Check Topics - this is kept seperate for the next models.\n",
    "        ############################################################\n",
    "    \n",
    "        #######################\n",
    "        # Extract the batch number and avro bytes\n",
    "        batch_number = int(msg.key())\n",
    "        avro_bytes = msg.value()\n",
    "    \n",
    "        #######################\n",
    "        # Check if topics are from \"batch-network-data\" or \"label-batch-labels\"\n",
    "        if msg.topic() == \"batch-network-data\":\n",
    "            file_path = os.path.join(folder_path, f\"features_batch_{batch_number}.avro\")\n",
    "            data_type = \"features\"\n",
    "        if msg.topic() == \"test-batch-labels\":\n",
    "            file_path = os.path.join(folder_path, f\"label_batch_{batch_number}.avro\")\n",
    "            data_type = \"labels\"\n",
    "            \n",
    "        #######################\n",
    "        # Write to file\n",
    "        with open(file_path, \"wb\") as file:\n",
    "            file.write(avro_bytes)\n",
    "            \n",
    "        #######################\n",
    "        # Print batch confirmation messagge \n",
    "        print(f\"Batch {batch_number} {data_type} saveed to folder.\")\n",
    "        \n",
    "\n",
    "############################################################\n",
    "# Error handling and logging\n",
    "############################################################             \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")   \n",
    "    \n",
    "############################################################\n",
    "# Clean up and close\n",
    "# ############################################################        \n",
    "finally:\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a997c39",
   "metadata": {},
   "source": [
    "Previously Data Ingestion \n",
    "  - 10% data:\n",
    "      - 5 Batches - DONE\n",
    "      - 100 Batches - DONE\n",
    "      - 250 Batches - DONE\n",
    "  - Full Files\n",
    "      - 2500 Batches - DONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2e39c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab21475e",
   "metadata": {},
   "source": [
    "# Basic Consumer test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f428ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Kafka configuration\n",
    "# conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "#        'group.id':'jupyter-consumer-group',\n",
    "#        'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# # Create a consumer instance\n",
    "# consumer = Consumer(conf)\n",
    "\n",
    "# # Subscribe to the topic\n",
    "# consumer.subscribe(['network-data-events'])\n",
    "\n",
    "# # Use a try, except, finally to allow kafka consumer to shut down gracefully.\n",
    "# # Try, continuously polls for new messages. Start the porducer to get a messages sent to the consumer\n",
    "# # except, lets you handle errors, \n",
    "# # Finally, lets you execte code. Lets the cosumer code be closed.\n",
    "\n",
    "# try:\n",
    "#     # Code block to poll messages from the producer\n",
    "#     while True:\n",
    "#         msg = consumer.poll(timeout=1.0)\n",
    "#         if msg is None: continue\n",
    "#         if msg.error():\n",
    "#             if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "#                 # End of partition event\n",
    "#                 print(f'{msg.topic()}[{msg.partition()}] reached end at offset {msg.offset()}')\n",
    "#             elif msg.error():\n",
    "#                 raise KafkaException(msg.error())\n",
    "#         else:\n",
    "#             print(f'Recieved message: {msg.value().decode(\"utf-8\")}')\n",
    "# except KeybaordInterrupt:\n",
    "#     pass\n",
    "# finally:\n",
    "#     # Closes the connection\n",
    "#     consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf224e",
   "metadata": {},
   "source": [
    "Start the consumer first to be listening for a producer message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74199ae",
   "metadata": {},
   "source": [
    "### SQL connection - NOT USED IN THIS CONSUMER NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812b293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.270981Z",
     "start_time": "2023-11-21T22:28:30.257660Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Function to connect to MYSQL connection\n",
    "# def mysql_connection():\n",
    "#     return pymysql.connect(host='localhost',\n",
    "#                             user='root',\n",
    "#                             password='root',\n",
    "#                             db='mtu_capstone_db')            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add1c4a4",
   "metadata": {},
   "source": [
    "### Save Model - - Some models are very large so they are not saved. NOT USED NOW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98644e",
   "metadata": {},
   "source": [
    "Models are too large to save, they will be 600mb plus times 50. I don't have the resources for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1890bfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.333842Z",
     "start_time": "2023-11-21T22:28:30.318772Z"
    }
   },
   "outputs": [],
   "source": [
    "# def save_model(model, filename):\n",
    "#     model_path = os.path.join('C:\\\\Users\\\\Kolobane\\\\OneDrive\\\\CIT MSc Data Science Modules\\\\_Semester Three - Final Project\\\\Project Two - Network Project\\\\ML Models\\\\Random Forest Real Time Models', filename)\n",
    "#     with open(filename, 'wb') as file:\n",
    "#         pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52eee6e",
   "metadata": {},
   "source": [
    "### Different attemps to serialise data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f550ac3",
   "metadata": {},
   "source": [
    "##### Attempt 1. Didnt work. Match error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfc710",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.349634Z",
     "start_time": "2023-11-21T22:28:30.334893Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Take the avro objects from the producer and with the schema, remove the data for use.\n",
    "# def deserialise_avro_record(avro_bytes, avro_schema):\n",
    "#     # Create a datum reader using the schema avro\n",
    "#     reader = DatumReader(avro_schema)\n",
    "    \n",
    "#     # Create a a Binary Decoder with avro bytes\n",
    "#     decoder = BinaryDecoder(io.BytesIO(avro_bytes))\n",
    "    \n",
    "#     return reader.read(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf7e78",
   "metadata": {},
   "source": [
    "##### Attempt 2: Create two functions, by listing out the featues in the DatumReader did not work. Deleted code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96035188",
   "metadata": {},
   "source": [
    "###### Attempt 3: Getting a \"Received message from topic: batch-network-data, key: b'0'. Error: unhashable type: 'RecordSchema'\" error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1784252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.365497Z",
     "start_time": "2023-11-21T22:28:30.350640Z"
    }
   },
   "outputs": [],
   "source": [
    "# def deserialise_features_avro_record(avro_bytes, features_avro_schema):\n",
    "#     # Create a by bytesIO steam from the avro bytes\n",
    "#     bytes_reader = io.BytesIO(avro_bytes)\n",
    "    \n",
    "#     # deserailise the data using the schema\n",
    "#     features_data = schemaless_reader(bytes_reader, features_avro_schema)\n",
    "    \n",
    "#     return features_data\n",
    "\n",
    "# def deserialise_label_avro_record(avro_bytes, label_avro_schema):\n",
    "#     # Create a by bytesIO steam from the avro bytes\n",
    "#     bytes_reader = io.BytesIO(avro_bytes)\n",
    "    \n",
    "#     # deserailise the data using the schema\n",
    "#     label_data = schemaless_reader(bytes_reader, label_avro_schema)\n",
    "    \n",
    "#     return label_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e77c9",
   "metadata": {},
   "source": [
    "##### Attempt 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc81656",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T12:24:34.614949Z",
     "start_time": "2023-11-22T12:24:34.602430Z"
    }
   },
   "outputs": [],
   "source": [
    "# def deserialise_features_avro_record(avro_bytes, schema):\n",
    "#     bytes_reader = io.BytesIO(avro_bytes)\n",
    "#     deserialised_data = []\n",
    "#     for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "#         deserialised_data.append(record)\n",
    "#     return deserialised_data\n",
    "\n",
    "# def deserialise_label_avro_record(avro_bytes, schema):\n",
    "#     bytes_reader = io.BytesIO(avro_bytes)\n",
    "#     deserialised_data = []\n",
    "#     for record in fastavro.reader(bytes_reader, reader_schema=schema):\n",
    "#         deserialised_data.append(record)\n",
    "#     return deserialised_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3894f",
   "metadata": {},
   "source": [
    "### Run Consumer 1: Random Forest Basic Model - Work in Progress - NOT USED."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e170086",
   "metadata": {},
   "source": [
    "Set up consumer to listen for the batches sent from the producer. Loads in baseline model, tests the batches, outputs results to database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe15a65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-21T22:28:30.397249Z",
     "start_time": "2023-11-21T22:28:30.382717Z"
    }
   },
   "outputs": [],
   "source": [
    "# ############################################################\n",
    "# # Set up the Kafka configuration\n",
    "# ############################################################\n",
    "# # Kafka configuration\n",
    "# conf = {'bootstrap.servers': 'localhost:9092', # Local kafka server address (local machine)\n",
    "#        'group.id':'jupyter-consumer-group',\n",
    "#        'auto.offset.reset': 'earliest'} # Start from the beginning of the topic. \n",
    "\n",
    "# # Create Kafka Consumer instance\n",
    "# consumer = Consumer(conf)\n",
    "\n",
    "# # Subscribe to the topics\n",
    "# consumer.subscribe([\"batch-network-data\", \"test-batch-labels\"]) # consumer needs to listen to both topics.\n",
    "\n",
    "# ############################################################\n",
    "# # Load the baseline model: Random Forest\n",
    "# ############################################################\n",
    "# # # Model Location\n",
    "# # model_path = r\"C:\\Users\\Kolobane\\OneDrive\\CIT MSc Data Science Modules\\_Semester Three - Final Project\\Project Two - Network Project\\ML Models\\rf_model_baseline_basic.joblib\"\n",
    "\n",
    "# # # Load the previous pre-trained baseline model.\n",
    "# # model = joblib.load(model_path)\n",
    "\n",
    "# ############################################################\n",
    "# # Random Forest model parameters\n",
    "# ############################################################\n",
    "# # # For each model I will use the same parameters as I used in the previous models. For RF basic model, thats n_estimators=100\n",
    "# # new_model_hyperparameters = {\n",
    "# #     'n_estimators': 100\n",
    "# # }\n",
    "\n",
    "# ############################################################\n",
    "# # Initialize dictionaries for storing batch data\n",
    "# ############################################################\n",
    "# batch_features = {}\n",
    "# batch_labels = {}\n",
    "\n",
    "# ############################################################\n",
    "# # Start Consumer polling\n",
    "# ############################################################\n",
    "# try: \n",
    "#     while True:\n",
    "#         msg = consumer.poll(1.0) # poll the first topic\n",
    "        \n",
    "#         if msg is None:\n",
    "#             continue\n",
    "#         if msg.error():\n",
    "#             print(f\"Consumer error: {msg.error()}\")\n",
    "#             continue   \n",
    "    \n",
    "#         ############################################################\n",
    "#         # Check Topics - this is kept seperate for the next models.\n",
    "#         ############################################################\n",
    "    \n",
    "#         #######################\n",
    "#         # Check if topics are from \"batch-network-data\"\n",
    "#         if msg.topic() == \"batch-network-data\":\n",
    "#             batch_number = int(msg.key()) # get the batch number\n",
    "#             avro_bytes = msg.value()\n",
    "            \n",
    "#             # Deserilise the avro data with the label_avro_schema loaded above\n",
    "#             avro_data = deserialize_avro_record(avro_bytes, features_avro_schema )\n",
    "#             batch_features[batch_number] = avro_data\n",
    "            \n",
    "    \n",
    "#         #######################\n",
    "#         # Check if topics are from \"test-batch-labels\"\n",
    "#         if msg.topic() == \"test-batch-labels\":\n",
    "#             batch_number = int(msg.key())\n",
    "#             avro_bytes = msg.value()\n",
    "            \n",
    "#             # Deserilise the avro data with the features_avro_schema loaded above\n",
    "#             avro_data = deserialize_avro_record(avro_bytes,label_avro_schema)\n",
    "#             batch_features[batch_number] = avro_data\n",
    "            \n",
    "\n",
    "#         ############################################################\n",
    "#         # Predict Using Baseline Models\n",
    "#         ############################################################\n",
    "    \n",
    "# #         #######################\n",
    "# #         # Use baseline model to make predictions with current batch\n",
    "# #         if batch_number in batch_features and batch_number in batch_labels:\n",
    "# #             batch_features_data = batch_features[batch_number]\n",
    "# #             batch_labels_data = batch_labels[batch_number]\n",
    "            \n",
    "# #             # Make predictions using the baseline model\n",
    "# #             baseline_predictions = model.predict(preprocessed_data)\n",
    "        \n",
    "# #         else:\n",
    "# #             print(\"Batch features or labels are missing for batch number:\", batch_number)\n",
    "        \n",
    "#         ############################################################\n",
    "#         # Combine Features and Labels\n",
    "#         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         # Combine Features and the labels.\n",
    "# #         if batch_features_data and batch_labels_data:\n",
    "# #             combined_data = {\n",
    "# #                 \"features\": batch_features_data,\n",
    "# #                 \"labels\": batch_labels_data\n",
    "# #             }\n",
    "# #         else:\n",
    "# #             print(\"Batch features or labels are missing for batch number:\", batch_number)\n",
    "        \n",
    "#         ############################################################\n",
    "#         # Evaluate Predictions\n",
    "#         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## Compare the baseline model with the actual labels.\n",
    "# #         if batch_number in baseline_predictios and batch_number in batch_labels:\n",
    "# #             baseline_predictions = baseline_predictions_dict[batch_number]\n",
    "# #             actual_labels = batch_labels[batch_number]\n",
    "            \n",
    "# #             if baseline_predictions and actual_labels:\n",
    "                \n",
    "# #                 # Metrics\n",
    "# #                 accuracy_value = accuracy_score(actual_labels, baseline_predictions)\n",
    "# #                 precision_value = precision_score(actual_labels, baseline_predictions)\n",
    "# #                 recall_value = recall_score(actual_labels, baseline_predictions)\n",
    "# #                 f1_score_value = f1_score(actual_labels, baseline_predictions)\n",
    "# #                 auc_score = roc_auc_score(actual_labels, baseline_predictions)\n",
    "                \n",
    "# #                 # Confusion matrix\n",
    "# #                 confusion_matrix_result = confusion_matrix(actual_labels, baseline_predictions)\n",
    "# #                 confusion_matrix = json.dumps(confusion_matrix, matrix_result.tolist())\n",
    "                \n",
    "# #                 # feature Importance\n",
    "# #                 feature_importance_results = model.feature_importances_\n",
    "# #                 feature_importance = json.dumps(feature_importance_results)           \n",
    "                \n",
    "# #             else:\n",
    "# #                 print(\"Batch data is empty for batch_number:\", batch_number)\n",
    "        \n",
    "# #         else:\n",
    "# #             print(\"Batch predictions or labels missing in batch_number:\", batch_number)\n",
    "        \n",
    "        \n",
    "# #         ############################################################\n",
    "# #         # Retrain the baseline model to get the next baseline model.\n",
    "# #         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## retain model to get new baseline model\n",
    "# #         retrain_start_time = time.time()\n",
    "        \n",
    "# #         # Create a new baseline model with the same hyperparameters\n",
    "# #         new_baseline_model = RandomForestClassifier(**new_model_hyperparameters)\n",
    "        \n",
    "# #         # Retrain the baseline model wiht the batch data\n",
    "# #         new_baseline_model.fit(batch_features_data, batch_labels_data)\n",
    "        \n",
    "# #         # Stop the timer\n",
    "# #         retrain_end_time =  time.time()\n",
    "        \n",
    "# #         #######################\n",
    "# #         # save date\n",
    "# #         model_training_time_seconds = retrain_end_time -retrain_start_time\n",
    "        \n",
    "# #         # save the new model parameters # although I have decided to use the same.\n",
    "# #         new_model_parameters_results = new_baseline_model.get_params()\n",
    "# #         new_model_parameters_results_tolist = new_model_parameters_results.tolist()\n",
    "# #         new_model_parameters = json.dumps(new_model_parameters_results_tolist)\n",
    "        \n",
    "# #         ############################################################\n",
    "# #         # Send values to database\n",
    "# #         ############################################################\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## connect to the database\n",
    "# #         conn = mysql_connection()        \n",
    "# #         cursor = conn.cursor()\n",
    "        \n",
    "# #         #######################\n",
    "# #         ## Create SQL Query\n",
    "# #         insert_query = \"\"\"\n",
    "# #             INSERT INTO rf_basic_rt_model_results(\n",
    "# #                 batch_number, model_name, timestamp, accuracy_value,\n",
    "# #                 precision_value, recall_value, f1_value, auc_score,\n",
    "# #                 confusion_matrix, feature_importance, testing_time_seconds,\n",
    "# #                 model_training_time_seconds, model_parameters\n",
    "# #             )\n",
    "# #             VALUES (%s, %s, NOW(), %s, %s, %s, %s, %s, %s, %s, %s, %s, %s ) \"\"\"\n",
    "\n",
    "# #         # Values to be insert#\n",
    "# #         values =  (\n",
    "# #             batch_number, model_name, accuracy_value,\n",
    "# #             precision_value, recall_value, f1_value, auc_score,\n",
    "# #             confusion_matrix_json, feature_importance_json, testing_time_seconds,\n",
    "# #             retraining_time_seconds, new_model_parameters_json\n",
    "# #         )\n",
    "\n",
    "# #         # Execute the insert query that is defind above\n",
    "# #         cursor.execute(insert_query, values)\n",
    "        \n",
    "# #         # Commit the changes to the database\n",
    "# #         conn.commit()\n",
    "        \n",
    "# #         print(f\"Batch {batch_number} values inserted successfully into the database.\")\n",
    "       \n",
    "              \n",
    "# ############################################################\n",
    "# # Error handling and logging\n",
    "# ############################################################             \n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")\n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "    \n",
    "# ############################################################\n",
    "# # Clean up and close\n",
    "# ############################################################        \n",
    "# finally:\n",
    "#     # Good practices to reduce issues by cleaning up and closing connections\n",
    "              \n",
    "#     if \"conn\" in locals() or \"conn\" in globals():    \n",
    "#         conn.rollback()\n",
    "#         conn.close()\n",
    "        \n",
    "#     if \"cursor\" in locals() or \"conn\" in globals():   \n",
    "#         cursor.close()\n",
    "    \n",
    "#     consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f0b37",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bscapstone)",
   "language": "python",
   "name": "bscapstone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "333.767px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
